---
title: "Logistic Regression"
date: "Last updated on `r Sys.Date()`"
format: html
editor: visual
theme: cosmo
number-sections: true
---

```{r setup, include=FALSE}
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE, 
  fig.width = 16/2, 
  fig.height = 9/2
)

# Load all your used packages here:
library(tidyverse)
library(janitor)
library(broom)
library(ggplot2)
library(dplyr)
library(pROC)
library(caret)

# Load data
training <- read_csv("data/cs-training.csv", show_col_types = FALSE) %>% 
  clean_names() %>% 
  rename(Id = x1)
test <- read_csv("data/cs-test.csv", show_col_types = FALSE) %>% 
  clean_names() %>% 
  rename(Id = x1)
sample_submission <- read_csv("data/sampleEntry.csv", show_col_types = FALSE) %>% 
  clean_names()
```

# Instructions

Participate in Kaggle's [Give Me Some Credit](https://www.kaggle.com/c/GiveMeSomeCredit) competition, where among other things you will:

1.  Fit a logistic regression model to predict who will experience financial distress in the next two years `serious_dlqin2yrs` as a function of
    1.  `age`
    2.  `number_of_open_credit_lines_and_loans`
2.  Compute Area under the ROC curve (AUC) as your score
3.  Perform crossvalidation to obtain an estimate of the Kaggle score

## Suggested workflow

1.  Render this file and read all instructions
2.  On Moodle, submit a `.zip` compressed/archived file of this **entire** RStudio project folder. We are doing this to ensure the graders can reproduce your Quarto file.

## Evaluation criteria

Grading will be done on a progressive scale:

1.  Minimum Viable Product Phase 1 - EDA (6/10)
    1.  Did you complete honor code section?
    2.  Does your Quarto file knit? Submissions that don't knit for users other than yourself will be penalized harshly. So render early, render often, and ensure reproducibility.
    3.  All questions in this phase
2.  Due Diligence Phase 2 - Fit and submit AUC (7.5/10)
3.  Phase 3 - Cross validation (9/10)
4.  Phase 4 - Compare scores (9.5/10)
5.  Reach for the stars Phase 5 - Linear decision boundary (10/10)

# Honor Code

-   Indicate anybody you collaborated with:
-   Indicate that you did not directly copy anybody else's code: I did not directly copy anybody else's code.
-   Indicate that you did not use ChatGPT for this problem set: I did not use ChatGPT for this problem set.

# MVP Phase 1: Exploratory data analysis

Conduct an exploratory data analysis to answer the following questions using informative data visualizations and simple summary statistics:

1.  If you guessed `serious_dlqin2yrs` is `TRUE` for all observations, how often do you expect to be right?
2.  Does `serious_dlqin2yrs` have a positive or negative relationship with `age`?
3.  Does `serious_dlqin2yrs` have a positive or negative relationship with `number_of_open_credit_lines_and_loans`?
4.  Optional: Create a single visualization that shows the relationship of all three variables

First, let's convert `serious_dlqin2yrs` to a boolean variable

```{r}
training$serious_dlqin2yrs_logical <- as.logical(training$serious_dlqin2yrs)
```

## EDA of outcome variable

```{r}
ggplot(data = training, aes(x = serious_dlqin2yrs_logical)) +
  geom_bar(stat = "count") +
  geom_text(stat = "count", aes(label = ..count..), vjust = -0.5) +
  labs(x = 'In Financial Distress?', y = 'Count', title = 'Count of People Predicted to Experience Financial Distress in the Next Two Years') +
  scale_y_continuous(labels = scales::comma) +
  theme_classic()
```

10026 (number of y = TRUE) / 150000 (total obs) \* 100 = 6.68%

If I guessed `serious_dlqin2yrs` is `TRUE` for all observations, I expect to be right for 10026 observations, or 6.68% of the time.

## EDA of outcome variable vs age

```{r}
ggplot(data = training, aes(x = serious_dlqin2yrs_logical, y = age)) +
  geom_boxplot() +
  labs(x = 'In Financial Distress?', y = 'Age', title = 'Ages of People Predicted to Experience Financial Distress in the Next Two Years') +
  theme_classic()
```

```{r}
ggplot(data = training, aes(x = age, y = serious_dlqin2yrs)) +
  # Training data with black points:
  geom_jitter(height = 0.05) +
  # Best fitting linear regression line in blue:
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Age", y = "In financial distress?")
```

```{r}
age_financial_distress <- lm(data = training, serious_dlqin2yrs ~ age)

summary(age_financial_distress)
```

`serious_dlqin2yrs` has a negative relationship with `age` because the fitted line in the plot has a negative correlation and the slope in the table of summary statistics is negative.

## EDA of outcome variable vs credit lines

```{r}
ggplot(data = training, aes(x = serious_dlqin2yrs_logical, y = number_of_open_credit_lines_and_loans)) +
  geom_boxplot() +
  labs(x = 'In Financial Distress?', y = 'Number of Open Credit Lines and Loans', title = 'Number of Credit Lines of People Predicted to Experience Financial Distress in the Next Two Years') +
  #scale_y_continuous(labels = scales::comma) +
  theme_classic()
```

```{r}
ggplot(data = training, aes(x = number_of_open_credit_lines_and_loans, y = serious_dlqin2yrs)) +
  # Training data with black points:
  geom_jitter(height = 0.05) +
  # Best fitting linear regression line in blue:
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Number of Open Credit Lines and Loans", y = "In financial distress?")
```

```{r}
credit_lines_financial_distress <- lm(data = training, serious_dlqin2yrs ~ number_of_open_credit_lines_and_loans)

summary(credit_lines_financial_distress)
```

`serious_dlqin2yrs` has a negative relationship with `number_of_open_credit_lines_and_loans` because the fitted line in the plot has a negative correlation and the slope in the table of summary statistics is negative.

## Optional: EDA of outcome variable vs both predictor variables

```{r}
ggplot(data = training, aes(x = age, y = number_of_open_credit_lines_and_loans, color = serious_dlqin2yrs_logical)) +
  geom_point() +
  labs(x = 'Age', y = 'Number of Open Credit Lines and Loans', title = 'Number of Credit Lines and Ages of People Predicted to Experience Financial Distress in the Next Two Years') +
  theme_classic()
```

# Due Diligence Phase 2: Logistic regression model

1.  Fit the corresponding logistic regression model to all the training data. Use the `broom::tidy()` function on the fitted model with `conf.int=TRUE` to show the confidence intervals. You can sanity check your answers to the EDA with these values.
2.  Obtain the AUC score of the model on the training data
3.  Predict the outcome for the test data and get a Kaggle score. Your score is the private score, not the public one.

Notes:

1.  You will add all AUC scores to a table later on.
2.  I suggest you do an EDA of your predicted/fitted values if you encounter issues

## Fit Model

```{r}
# Fit model
model_logistic <- glm(serious_dlqin2yrs ~ age + number_of_open_credit_lines_and_loans, data = training, family = "binomial")

summary(model_logistic)

# Extract regression table with confidence intervals
model_logistic %>%
  broom::tidy(conf.int = TRUE)
```

## Get AUC score

```{r}
training$y_hat <- predict(model_logistic, newdata = training, type = "response")

auc <- auc(roc(training$serious_dlqin2yrs, training$y_hat))
print(auc)
```

## Predict the outcome for test data

```{r}
test$y_prob <- predict(model_logistic, newdata = test, type = "response")
```

## Make submission to Kaggle

```{r}
submission_logistic <- test |>
  select(Id, y_prob)

submission_logistic <- rename(submission_logistic, Probability = y_prob)

submission_logistic

write_csv(submission_logistic, path = "data/submission_logistic.csv")
```

![](images/auc_logistic.png)

# Phase 3: Cross validation

Use $k=5$ fold cross-validation with a seed value of 76 to set the folds and obtain a local estimate of the Kaggle score. Normally when competing one would compute and cross-validation estimated score first and then compare it to Kaggle true score after, but here we are merely studying rather than actually competing.

```{r}
#training <- training[, !(names(training) %in% c("index", "index.1"))]

set.seed(76)

num_folds <- 5

folds <- createFolds(training$serious_dlqin2yrs, k = num_folds, list = TRUE, returnTrain = FALSE)

# initialize a vector to hold k-fold numbers 
index <- rep(NA, nrow(training))
# iterate along folds 
for (i in seq_along(folds)) {
  
  # add fold numbers to index vector 
  index[folds[[i]]] <- i
}

# add the index column to your dataset
training <- cbind(index = index, training)

# initialize vector to hold 5 auc scores of test_validation
auc_score <- rep(0, num_folds)

for (i in 1:num_folds) {
  
  # initialize test and train validation 
  test_validation_k_fold <- training |>
    filter(index == i)
  train_validation_k_fold <- training |>
    filter(index != i)
  
  model_logistic_CV <- glm(serious_dlqin2yrs ~ age + number_of_open_credit_lines_and_loans, data = train_validation_k_fold, family = "binomial")
  
  test_validation_k_fold$y_hat <- predict(model_logistic_CV, newdata = test_validation_k_fold, type = "response")

  auc <- auc(roc(test_validation_k_fold$serious_dlqin2yrs, test_validation_k_fold$y_hat))
  
  auc_score[i] <- auc
}

avg_auc <- mean(auc_score)
print(avg_auc)
```

# Phase 4: Compare scores

Fill in this table with the three AUC's of your fitted model you computed

1.  Would you say the three scores are similar or very different? The three scores are very similar.
2.  Why? The score on the training data is just slightly better than the one predicted on test data, which means that the model is fitted well on the training data and thus, generalizes well to unseen test data. The 5-fold estimated AUC is close to the AUC on training data, which confirms that the model on the training data is accurate and reliable.

| AUC on training data | AUC on test data from Kaggle | 5-fold Estimated AUC |
|----------------------|------------------------------|----------------------|
| 0.636                | 0.63418                      | 0.63597              |

# Reach for the stars Phase 5: Decision boundary

In the [Linear Classifiers in Python](https://app.datacamp.com/learn/courses/linear-classifiers-in-python) course, Chapter 1, Video 3, Exercise 1, top left graph in they illustrate a **linear decision boundary** that sets the boundary at which the predicted level of a binary variable switches from blue to red when you use a model with two numerical predictors $X_1$ on the x-axis and $X_2$ on the y-axis

![](images/decision_boundary.png){width="75%"}

Draw the linear decision boundary for our logistic regression model where:

1.  x-axis: `age` from 0 to the maximum age in the training data
2.  y-axis: `number_of_open_credit_lines_and_loans` from 0 to the maximum number of lines
3.  The decision boundary is where the fitted probability $\widehat{p}$ is 0.5

```{r}
# function to return a plot of the linear decision boundary 
get_linear_decision_boundary <- function(p_hat) {
  log_odds <- log(p_hat/(1-p_hat))

  max_age <- max(training$age)
  max_credit_lines <- max(training$number_of_open_credit_lines_and_loans)

  linear_boundary_points <- data.frame(age = 0:max_age)

  linear_decision_boundary <- linear_boundary_points |>
    select(age) |>
    mutate(credit_lines = (log_odds-(model_logistic$coefficients['(Intercept)'] + (model_logistic$coefficients['age'] * age)))/(model_logistic$coefficients['number_of_open_credit_lines_and_loans'])) |>
    filter(credit_lines > 0 & credit_lines < max_credit_lines)
  
  training$color <- ifelse(training$y_hat > p_hat, "red", "blue")

  plot <- ggplot(data = training, aes(x = age, y = number_of_open_credit_lines_and_loans)) + 
  geom_point(color = training$color) + 
  labs(x = 'Age', y = '# Open Credit Lines and Loans', title = 'Linear Decision Boundary')
  #plot

  # Add points from Dataset 2 to the plot
  plot <- plot + geom_line(data = linear_decision_boundary, aes(x = age, y = credit_lines), linewidth = 1)
  
  return(plot)
}
# get linear decision boundary with my chosen p_hat
get_linear_decision_boundary(0.0668)
```
